#!/usr/bin/env python3
"""
æ¥½å¤©å¸‚å ´APIå¤‰æ›´æ¤œå‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒšãƒ¼ã‚¸æ§‹é€ ã®å¤‰æ›´ã‚’æ¤œå‡ºã—ã€ãƒ¡ãƒ¼ãƒ«é€šçŸ¥ã‚’é€ä¿¡
"""

import json
import hashlib
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional
import logging
import requests
from bs4 import BeautifulSoup

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
log_dir = Path(os.getenv('DATA_DIR', '.')) / 'logs'
log_dir.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / f'api_detection_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç›£è¦–å¯¾è±¡URL
MONITORING_URLS = {
    "ranking": "https://ranking.rakuten.co.jp/daily/",
    "search": "https://search.rakuten.co.jp/search/mall/",
}

# é‡è¦ãªã‚»ãƒ¬ã‚¯ã‚¿
CRITICAL_SELECTORS = {
    "ranking": [
        "a.ranking-item",
        "span.price",
        "span.review-count",
        "span.rating",
    ],
    "search": [
        "a.item-name",
        "span.item-price",
        "span.item-review",
    ]
}

class APIChangeDetector:
    def __init__(self):
        self.data_dir = Path(os.getenv('DATA_DIR', '.')) / 'api-monitoring'
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.baseline_file = self.data_dir / 'baseline.json'
        self.change_log_file = self.data_dir / 'changes.log'
        self.load_baseline()
    
    def load_baseline(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã‚€"""
        if self.baseline_file.exists():
            with open(self.baseline_file, 'r', encoding='utf-8') as f:
                self.baseline = json.load(f)
        else:
            self.baseline = {}
    
    def save_baseline(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜"""
        with open(self.baseline_file, 'w', encoding='utf-8') as f:
            json.dump(self.baseline, f, ensure_ascii=False, indent=2)
    
    def get_page_hash(self, url: str, selectors: list) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã®ç‰¹å®šè¦ç´ ã®ãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŒ‡å®šã•ã‚ŒãŸã‚»ãƒ¬ã‚¯ã‚¿ã‹ã‚‰è¦ç´ ã‚’æŠ½å‡º
            elements_text = ""
            for selector in selectors:
                elements = soup.select(selector)
                for elem in elements[:5]:  # æœ€åˆã®5è¦ç´ ã®ã¿
                    elements_text += elem.get_text(strip=True)
            
            # ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            page_hash = hashlib.md5(elements_text.encode()).hexdigest()
            return page_hash
        
        except Exception as e:
            logger.error(f"ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {str(e)}")
            return None
    
    def detect_changes(self) -> Dict:
        """APIå¤‰æ›´ã‚’æ¤œå‡º"""
        changes = {
            "detection_time": datetime.now().isoformat(),
            "detected_changes": [],
            "status": "ok"
        }
        
        logger.info("=" * 70)
        logger.info("ğŸ” æ¥½å¤©å¸‚å ´APIå¤‰æ›´æ¤œå‡ºã‚’é–‹å§‹")
        logger.info(f"â° æ¤œå‡ºæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S (JST)')}")
        logger.info("=" * 70 + "\n")
        
        for page_name, url in MONITORING_URLS.items():
            logger.info(f"ğŸ“ ç›£è¦–ä¸­: {page_name}")
            
            selectors = CRITICAL_SELECTORS.get(page_name, [])
            current_hash = self.get_page_hash(url, selectors)
            
            if current_hash is None:
                logger.warning(f"   âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—\n")
                continue
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒ
            baseline_hash = self.baseline.get(page_name)
            
            if baseline_hash is None:
                # åˆå›å®Ÿè¡Œ
                logger.info(f"   â„¹ï¸ åˆå›å®Ÿè¡Œ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜\n")
                self.baseline[page_name] = current_hash
            elif baseline_hash != current_hash:
                # å¤‰æ›´æ¤œå‡º
                logger.warning(f"   ğŸš¨ å¤‰æ›´æ¤œå‡º!")
                logger.warning(f"   å‰å›: {baseline_hash}")
                logger.warning(f"   ç¾åœ¨: {current_hash}\n")
                
                changes["detected_changes"].append({
                    "page": page_name,
                    "url": url,
                    "previous_hash": baseline_hash,
                    "current_hash": current_hash,
                    "detected_at": datetime.now().isoformat()
                })
                
                changes["status"] = "changes_detected"
                self.baseline[page_name] = current_hash
            else:
                logger.info(f"   âœ“ å¤‰æ›´ãªã—\n")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜
        self.save_baseline()
        
        # å¤‰æ›´ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_changes(changes)
        
        logger.info("=" * 70)
        if changes["status"] == "changes_detected":
            logger.warning(f"âš ï¸ APIå¤‰æ›´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ!")
            logger.warning(f"   æ¤œå‡ºä»¶æ•°: {len(changes['detected_changes'])}")
            logger.warning(f"   ãƒ¡ãƒ¼ãƒ«é€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã™")
        else:
            logger.info(f"âœ“ APIå¤‰æ›´ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        logger.info("=" * 70 + "\n")
        
        return changes
    
    def log_changes(self, changes: Dict):
        """å¤‰æ›´ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        with open(self.change_log_file, 'a', encoding='utf-8') as f:
            f.write(f"\n{json.dumps(changes, ensure_ascii=False, indent=2)}\n")
    
    def run(self) -> bool:
        """æ¤œå‡ºã‚’å®Ÿè¡Œ"""
        changes = self.detect_changes()
        
        # å¤‰æ›´ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã§çµ‚äº†ï¼ˆãƒ¡ãƒ¼ãƒ«é€šçŸ¥ãƒˆãƒªã‚¬ãƒ¼ï¼‰
        if changes["status"] == "changes_detected":
            return False
        
        return True

if __name__ == "__main__":
    detector = APIChangeDetector()
    success = detector.run()
    exit(0 if success else 1)
